\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=.7in]{geometry}
\usepackage{listings}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multicol}
\usepackage{graphicx}
\graphicspath{{./Figures/}}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=blue,
}
\titleformat*{\section}{\LARGE\bfseries\filcenter}
\titleformat*{\subsection}{\Large\bfseries}
\titleformat*{\subsubsection}{\large\bfseries}
\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codered}{rgb}{0.78,0,0}
\definecolor{codepurple}{rgb}{0.58,0,0.68}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{Pythonstyle}{
	language = Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{gray},
    keywordstyle=\color{codegreen},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codered},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    morekeywords = {as},
    keywordstyle = \color{codegreen}
}
\lstset{style=Pythonstyle}

\begin{document}
	\begin{titlepage}
		\begin{center} \Huge \textbf{DeepLearning.AI TensorFlow Developer} \end{center}
		\tableofcontents
		\newpage
	\end{titlepage}
%%%% PAGE 1 %%%%

	\begin{spacing}{1.1}
	\section{Introduction to TensorFlow for AI, ML, and DL}
	\subsubsection{Callbacks}
	We can use \textbf{callbacks} in order to stop training when we reach a certain accuracy we desire. This is to stop the loss from beginning to increase again if we start to overfit the model. \href{https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback}{Click here} to see the TensorFlow Callbacks documentation.
	\begin{lstlisting}
	import tensorflow as tf
	print(tf.__version__)
	
	class myCallback(tf.keras.callbacks.Callback):
		def on_epoch_end(self, epoch, logs={}):
			if(logs.get('accuracy')>0.6): # might need to use 'acc' instead
				print("\nReached 60% accuracy so cancelling training!")
				self.model.stop_training = True
	
	callbacks = myCallback()
	
	mnist = tf.keras.datasets.fashion_mnist
	(x_train, y_train),(x_test, y_test) = mnist.load_data()
	x_train, x_test = x_train / 255.0, x_test / 255.0
	
	model = tf.keras.models.Sequential([
		tf.keras.layers.Flatten(),
		tf.keras.layers.Dense(512, activation=tf.nn.relu),
		tf.keras.layers.Dense(10, activation=tf.nn.softmax)
	])
	
	model.compile(optimizer='adam', 
	              loss='sparse_categorical_crossentropy',
	              metrics=['accuracy'])
	model.fit(x_images, y_labels, epochs=10, callbacks=[callbacks]) \end{lstlisting}\vspace*{1mm}

	\subsubsection{Upload Custom Images}
	We can use the below code to upload a custom image and use it on a trained model.
	\begin{lstlisting}
	import numpy as np
	from google.colab import files
	from keras.preprocessing import image
	
	uploaded = files.upload()
	
	for fn in uploaded.keys():
		# predicting images
		path = '/content/' + fn
		img = image.load_img(path, target_size=(300, 300))
		x = image.img_to_array(img)
		x = np.expand_dims(x, axis=0)
		
		images = np.vstack([x])
		classes = model.predict(images, batch_size=10)
		print(classes[0])
		if classes[0]>0.5:
			print(fn + " is a human")
		else:
			print(fn + " is a horse") \end{lstlisting} \newpage
%%%% PAGE 2 %%%%

	\subsubsection{ImageDataGenerator}
	\begin{lstlisting}
	import tensorflow as tf
	import os
	import zipfile
	from os import path, getcwd, chdir
	from tensorflow.keras.optimizers import RMSprop
    from tensorflow.keras.preprocessing.image import ImageDataGenerator	
	
	# Import and extract zip file containing images
	path = f"{getcwd()}/../tmp2/happy-or-sad.zip"
	zip_ref = zipfile.ZipFile(path, 'r')
	zip_ref.extractall("/tmp/h-or-s")
	zip_ref.close()
	
	def train_happy_sad_model():		
		DESIRED_ACCURACY = 0.999
		
		class myCallback(tf.keras.callbacks.Callback):
			def on_epoch_end(self, epoch, logs={}):
				if(logs.get('acc')>DESIRED_ACCURACY):
					print('\nReached 100% accuracy so stopping training.')
					self.model.stop_training = True
		
		callbacks = myCallback()
		
		# Define and Compile the Model.
		model = tf.keras.models.Sequential([
				tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3)),
				tf.keras.layers.MaxPooling2D(2,2),
				tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
				tf.keras.layers.MaxPooling2D(2,2),
				tf.keras.layers.Conv2D(16, (3,3), activation='relu'),
				tf.keras.layers.MaxPooling2D(2,2),
				tf.keras.layers.Flatten(),
				tf.keras.layers.Dense(512),
				tf.keras.layers.Dense(1, activation='sigmoid')
		])
		
		model.compile(optimizer=RMSprop(lr=0.001),
		              loss='binary_crossentropy',
		              metrics=['accuracy'])
		
		# Create an instance of an ImageDataGenerator 		
		train_datagen = ImageDataGenerator(rescale=1./255)
		
		train_generator = train_datagen.flow_from_directory(
				'/tmp/h-or-s', # directory containing the images
				target_size=(150,150),
				class_mode='binary'
		)

		history = model.fit(
				train_generator, # data generator object
				epochs=50,
				callbacks=[callbacks],
				verbose=1
		)
		
		return history.history['acc'][-1] \end{lstlisting}\newpage
%%%% PAGE 3 %%%%

	\section{CNN's in TensorFlow}
	\subsubsection{Using OS and Splitting Data}
	\begin{lstlisting}
	path_cats_and_dogs = f"{getcwd()}/../tmp2/cats-and-dogs.zip"
	shutil.rmtree('/tmp')
	
	local_zip = path_cats_and_dogs
	zip_ref = zipfile.ZipFile(local_zip, 'r')
	zip_ref.extractall('/tmp')
	zip_ref.close()
	
	try: # Make directories for training and testing, along with class subdirectories
		os.mkdir('/tmp/cats-v-dogs/')
		os.mkdir('/tmp/cats-v-dogs/training/')
		os.mkdir('/tmp/cats-v-dogs/testing/')
		os.mkdir('/tmp/cats-v-dogs/training/cats/')
		os.mkdir('/tmp/cats-v-dogs/testing/cats/')
		os.mkdir('/tmp/cats-v-dogs/training/dogs/')
		os.mkdir('/tmp/cats-v-dogs/testing/dogs/')
	except OSError:
		pass
		
	def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):
		"""
		a SOURCE directory containing the files
		a TRAINING directory that a portion of the files will be copied to
		a TESTING directory that a portion of the files will be copie to
		a SPLIT SIZE to determine the portion
		"""
		source_list = os.listdir(SOURCE) # get list of files
		random.sample(source_list, len(source_list)) # shuffle the list
		train_images = source_list[:int(len(source_list)*SPLIT_SIZE)]
		testing_images = source_list[int(len(source_list)*SPLIT_SIZE):]
		
		for img in train_images:
			if os.path.getsize(SOURCE+img) != 0: # make sure not empty file
				copyfile(SOURCE+img, TRAINING+img)
		
		for img in testing_images:
			if os.path.getsize(SOURCE+img) != 0: # make sure not empty file
				copyfile(SOURCE+img, TESTING+img)
		
		return None
	
	CAT_SOURCE_DIR = "/tmp/PetImages/Cat/"
	TRAINING_CATS_DIR = "/tmp/cats-v-dogs/training/cats/"
	TESTING_CATS_DIR = "/tmp/cats-v-dogs/testing/cats/"
	DOG_SOURCE_DIR = "/tmp/PetImages/Dog/"
	TRAINING_DOGS_DIR = "/tmp/cats-v-dogs/training/dogs/"
	TESTING_DOGS_DIR = "/tmp/cats-v-dogs/testing/dogs/"
	
	split_size = .9
	split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)
	split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size) \end{lstlisting} \newpage
%%%% PAGE 4 %%%% 

	\subsubsection{Building a CNN with IDG}
	\begin{lstlisting}
	model = tf.keras.models.Sequential([
			tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),
			tf.keras.layers.MaxPooling2D(2,2),
			tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
			tf.keras.layers.MaxPooling2D(2,2),
			tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
			tf.keras.layers.MaxPooling2D(2,2),
			tf.keras.layers.Flatten(),
			tf.keras.layers.Dense(512, activation='relu'),
			tf.keras.layers.Dense(1, activation='sigmoid')
	])
	
	model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', 
								metrics=['acc'])
	
	# Feed training data into our IDG object
	TRAINING_DIR = '/tmp/cats-v-dogs/training/'
	train_datagen = ImageDataGenerator(rescale=1./255,
	                                   rotation_range=45,
	                                   horizontal_flip=True,
	                                   vertical_flip=True,
	                                   shear_range=0.2,
	                                   zoom_range=0.2,
	                                   fill_mode='nearest')
	
	train_generator = train_datagen.flow_from_directory(TRAINING_DIR,
                                                      target_size=(150,150),
                                                      batch_size=10,
                                                      class_mode='binary')
	# Feed testing data into our IDG object
	VALIDATION_DIR = '/tmp/cats-v-dogs/testing/'
	validation_datagen = ImageDataGenerator(rescale=1./255)
	
	validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,
																			                          target_size=(150,150),
																			                          batch_size=10,
																			                          class_mode='binary')
	history = model.fit_generator(train_generator,
	                              epochs=2,
	                              verbose=1,
	                              validation_data=validation_generator)  \end{lstlisting} \vspace*{4mm}
	                             
	\subsubsection{Transfer Learning: Built-in Models}
	\begin{lstlisting}
	path_inception = f"{getcwd()}/../tmp2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
	
	# Import the inception model  
	from tensorflow.keras.applications.inception_v3 import InceptionV3
	
	# Create an instance of the inception model from the local pre-trained weights
	local_weights_file = path_inception
	
	pre_trained_model = InceptionV3(input_shape=(150,150,3), include_top=False, 
	                                weights=None)
	
	pre_trained_model.load_weights(local_weights_file) \end{lstlisting} \newpage
%%%% PAGE 5 %%%%

	\begin{lstlisting}
	# Make all the layers in the pre-trained model non-trainable
	for layer in pre_trained_model.layers:
		layer.trainable = False
		
	last_layer = pre_trained_model.get_layer('mixed7')
	print('last layer output shape: ', last_layer.output_shape) # (None, 7, 7, 768)
	last_output = last_layer.output	
	
	# Flatten the output layer to 1 dimension (with input from last pretrained layers)
	x = layers.Flatten()(last_output)
	
	x = layers.Dense(1024, activation='relu')(x)
	x = layers.Dropout(0.2)(x)                  
	x = layers.Dense(1, activation='sigmoid')(x)           
	
	model = Model(pre_trained_model.input, x) 
	
	model.compile(optimizer = RMSprop(lr=0.0001), 
	              loss = 'binary_crossentropy', 
	              metrics = ['acc']) \end{lstlisting} \vspace*{4mm}
	             
	\subsubsection{Reading Images from CSVs}
	\begin{lstlisting}
	def get_data(filename):
		"""
		Read the file passed into the function. The first line contains
		the header (so skip it). Each line contains 785 values, with
		the first being the label and remaining being the pixel values.
		You will need to reshape the images into 28x28.
		"""
		with open(filename) as training_file:
			labels = [] 
			images = []
			reader = csv.reader(training_file, delimiter = ',')
			next(reader, None) # skip first line
			
			for row in reader:
				labels.append(row[0])
				images.append(np.array(row[1:]).reshape(28,28))
			
		labels = np.array(labels).astype(float)
		images = np.array(images).astype(float)
		return images, labels
	
	path_sign_mnist_train = f"{getcwd()}/../tmp2/sign_mnist_train.csv"
	path_sign_mnist_test = f"{getcwd()}/../tmp2/sign_mnist_test.csv"
	training_images, training_labels = get_data(path_sign_mnist_train)
	testing_images, testing_labels = get_data(path_sign_mnist_test)
	
	print(training_images.shape) # (27455, 28, 28)
	print(training_labels.shape) # (27455)
	print(testing_images.shape) # (7172, 28, 28)
	print(testing_labels.shape) # (7172)
	
	# Expand dimensions (add 1 to the end)
	training_images = np.expand_dims(training_images, axis=3) #	(27455, 28, 28, 1)
	testing_images = np.expand_dims(testing_images, axis=3) # (7172, 28, 28, 1)	\end{lstlisting} \newpage
%%%% PAGE 6 %%%%

	\subsubsection{Multi-Class Classification}
	\begin{lstlisting}
	# Create an ImageDataGenerator objects
	train_datagen = ImageDataGenerator(
			rescale=1./255,
			rotation_range=45,
			horizontal_flip=True,
			vertical_flip=True,
			zoom_range=0.2 )
	
	validation_datagen = ImageDataGenerator(rescale=1./255)
	
	# Create generators (images already loaded, not from directory)
	train_generator = train_datagen.flow(training_images,
	                                     training_labels,
	                                     batch_size=32)
	
	valid_generator = validation_datagen.flow(testing_images,
	                                          testing_labels,
	                                          batch_size=32)
	
	# Define the model
	model = tf.keras.models.Sequential([
			tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),
			tf.keras.layers.MaxPooling2D(2,2),
			tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
			tf.keras.layers.MaxPooling2D(2,2),
			tf.keras.layers.Flatten(),
			tf.keras.layers.Dense(512, activation='relu'),
			tf.keras.layers.Dense(26, activation='softmax') # 26 classes
	])
	
	# Compile Model. 
	model.compile(optimizer='adam',
	              loss='sparse_categorical_crossentropy',
	              metrics=['acc'])
	          
	# Train the Model
	history = model.fit_generator(train_generator,
	                              validation_data=valid_generator,
	                              epochs=2,
	                              verbose=1)          
	
	# Access the metrics for plotting
	acc = history.history['acc']
	val_acc = history.history['val_acc']
	loss = history.history['loss']
	val_loss = history.history['val_loss'] \end{lstlisting} \newpage
%%%% PAGE 7 %%%%

	\section{Natural Language Processing}
	\subsubsection{...}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
\end{spacing}
\end{document}