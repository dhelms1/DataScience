\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=.7in]{geometry}
\usepackage{listings}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multicol}
\usepackage{fancybox}
\usepackage{graphicx}
\graphicspath{{./Figures/}}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=purple,
}
\titleformat*{\section}{\LARGE\bfseries\filcenter}
\titleformat*{\subsection}{\Large\bfseries}
\titleformat*{\subsubsection}{\large\bfseries}
\definecolor{codegreen}{rgb}{0,0.5,0.3}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codered}{rgb}{0.78,0,0}
\definecolor{codepurple}{rgb}{0.58,0,0.68}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{Pystyle}{
	language = Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{gray},
    keywordstyle=\color{black},
    numberstyle=\tiny\color{codepurple},
    stringstyle=\color{codered},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    morekeywords = {as},
    keywordstyle = \color{codegreen}
}
\lstset{style=Pystyle}
\tcbset{
	colbacktitle=red!50!white, 
	title=Example, 
	coltitle=black, 
	colback=white, 
	fonttitle=\bfseries
}

\begin{document}
	\begin{titlepage}
		\begin{center} \Huge \textbf{TensorFlow Developer Certificate Notes} \end{center}
		\tableofcontents
		\newpage
	\end{titlepage}
	
%%%% PAGE 1 %%%%
	
	\noindent \Large \textbf{Introduction}: \normalsize
	\begin{itemize}
		\item \textbf{tf.constant()} is not mutable, but \textbf{tf.Variable()} is by using the \textit{.assign()} method on the var object.
		\item You must set both the global \textbf{tf.random.set\_seed()} and function \textbf{seed=} parameter to get reproducible results for shuffle function.
		\item We can \textit{add dimensions} to a tensor whilst keeping the same information (\textit{newaxis} and \textit{expand\_dims} have same output).
	\begin{lstlisting}
	rank_3_tensor = rank_2_tensor[..., tf.newaxis] # "..." means "all dims prior to"
	rank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1) 
	tf.expand_dims(rank_2_tensor, axis=-1) # "-1" means last axis (2, 2, 1)  \end{lstlisting}
		\item \textbf{tf.reshape()} will change the shape in the order they appear (top left to bottom right) and \textbf{tf.transpose()} simply flips the matrix.
		\item We can reduce tensor sizes in memory by changing the datatype (i.e. float32 cast to float16). 
		\item We can perform aggregation on tensors by using \textbf{reduce()\_[action]} and using min, max, mean, sum, etc. We can also find positional arguments using \textbf{tf.argmin()} or \textbf{tf.argmax()}.
	\end{itemize} \vspace*{6mm}
	\noindent \Large \textbf{Neural Network Classification}: \normalsize
	\begin{itemize}
		\item We can create a \textbf{learning rate callback} to update our learning rate during training.
	\begin{lstlisting}
	# Create a learning rate scheduler callback
	lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 
	               1e-4 * 10**(epoch/20)) \end{lstlisting}
		\item Traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch.
		\item Note that learning rate exponentially increases as epochs increases.
		\item We can use a plot to determine the \textbf{ideal learning rate}, which we want to take the value where loss is still decreasing but not quite flattened out. It is the value around 10x smaller than the lowest point (refer to notebook for graph and point selection).
	\begin{lstlisting}
	lrs = 1e-4 * (10 ** (np.arange(100)/20))
	plt.figure(figsize=(10, 7))
	plt.semilogx(lrs, history.history["loss"]) # x-axis (lr) to be log scale \end{lstlisting}
	\end{itemize} \newpage

%%%% PAGE 2 %%%%
	
	\section{Transfer Learning}
	\subsection{Feature Extraction}
	\begin{itemize}
		\item We can log the performance of multiple models, then view and compare these models in a visual way on a \textbf{TensorBoard}. It saves a model's training performance to a specified \textit{log\_dir}.
	\begin{lstlisting}
	def create_tensorboard_callback(dir_name, experiment_name):
		log_dir = dir_name + "/" + experiment_name + "/" +
		          datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
		tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)
		print(f"Saving TensorBoard log files to: {log_dir}")
		return tensorboard_callback	\end{lstlisting}
		\item We can also save a model as it trains so you can stop training if needed and come back to continue off where you left using \textbf{Model Checkpointing}. By default, metric monitored is \textit{validation loss}.
	\begin{lstlisting}
	cp_path = "model_checkpoint_name_here/checkpoint.ckpt"
	
	# Create a ModelCheckpoint callback that saves the model's weights only
	checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=cp_path,
		save_weights_only=True, # False to save the entire model
		save_best_only=False, # True to save only best model instead of every epoch 
		save_freq="epoch", # save every epoch
		verbose=1)	\end{lstlisting}
		\item \textbf{Feature Extraction} is when you take the weights a pretrained model has learned and adjust its outputs to be more suited to your problem (keep layers frozen except new output layers).
	\end{itemize} \vspace*{2mm}

	\subsection{Fine Tuning}
	\begin{itemize}
		\item The \textbf{GlobalAveragePooling2D} layer take the average of the outputs of the model (across the inner axis) and reduces it into a \textbf{feature vector} that is then passed to our final \textbf{Dense} layer, which then gives us our final output. For example, a tensor of shape (2, 4, 5, 3) will be reduced into shape (2, 3).
		\item Images are best preprocessed on the GPU where as text and structured data are more suited to be preprocessed on the CPU. Image data augmentation only happens during training so we can still export our whole model and use it elsewhere. And if someone else wanted to train the same model as us, including the same kind of data augmentation, they could.
		\item We can create a \textbf{Data Augmentation} layer for our model using the Sequential API and the \textit{ tf.keras.layers.experimental.preprocessing} layers. Note that this layer is turned off for predicting.
	\begin{lstlisting}
	data_augmentation = keras.Sequential([
		preprocessing.RandomFlip("horizontal"),
		preprocessing.RandomRotation(0.2),
		... # zoom, width, rotation, normalize, etc.
	], name = "data_augmentation")
	
	input_shape = (224, 224, 3)
	base_model = tf.keras.applications.EfficientNetB0(include_top=False)
	base_model.trainable = False # freeze model layers
	
	inputs = layers.Input(shape=input_shape, name="input_layer")
	x = data_augmentation(inputs)
	x = base_model(x, training=False)
	x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)
	outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)
	model = keras.Model(inputs, outputs)	\end{lstlisting}
	\end{itemize} \newpage

%%%% PAGE 3 %%%%

	\begin{itemize}
		\item In \textbf{Fine Tuning} we will unfreeze deeper layers in the model in order to learn more problem specific features for our dataset. Generally, the amount we unfreeze is determined by how much data we have. 
		\item \href{https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb}{Click here} for how to resume training after unfreezing layers and plotting the history.
	\end{itemize} \vspace*{2mm}
	
	\subsection{Scaling Up}
	\begin{itemize}
		\item We can used \href{https://www.tensorflow.org/guide/mixed_precision}{\textbf{Mixed Precision}} in order to improve our models performance on GPU by using a mix of float32 and float16 data types to use less memory where possible and in turn run faster (using less memory per tensor means more tensors can be computed on simultaneously). Note that this doesn't work for all hardware (must have score of 7.0+, see \textit{supported hardware} in above link).
	\begin{lstlisting}
	from tensorflow.keras import mixed_precision
	
	# set global policy to mixed precision
	mixed_precision.set_global_policy(policy="mixed_float16") \end{lstlisting}
		\item Note that in the final output layer, it is required to specify the \textit{dtype=tf.float32} and use the \textbf{Activation} layer instead of Dense when using mixed precision.
	\begin{lstlisting}
	base_model = tf.keras.applications.EfficientNetB0(include_top=False)
	base_model.trainable = False # freeze base model layers

	inputs = layers.Input(shape=input_shape, name="input_layer")
	x = base_model(inputs, training=False) # set base_model to inference mode only
	x = layers.GlobalAveragePooling2D(name="pooling_layer")(x)
	x = layers.Dense(len(class_names))(x) # want one output neuron per class 
	# Separate activation of output layer so we can output float32 activations
	outputs = layers.Activation("softmax", dtype=tf.float32, name="sm_float32")(x) 
	model = tf.keras.Model(inputs, outputs)	

	for layer in model.layers:
		print(layer.dtype_policy) # Check the dtype policy of layers\end{lstlisting}
	\end{itemize} \newpage

%%%% PAGE 4 %%%%

	\section{Natural Language Processing}
	\begin{itemize}
		\item \textbf{Text Vectorization Layer} - maps input sequence to numbers (convert words to number pairing).
		\item \textbf{Embedding} - Turns mapping of text vectors to embedding matrix (finds how words relate).
		\item \textbf{RNN cell(s)} - find patterns in sequences (usually an \textit{LSTM} layer with \textit{tanh} activation).
	\end{itemize}
	
	
	
	
	
\end{document}